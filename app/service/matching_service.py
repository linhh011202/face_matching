"""
Face Matching - Enhanced Ensemble Approach
==========================================
Káº¿t há»£p nhiá»u ká»¹ thuáº­t Ä‘á»ƒ matching chÃ­nh xÃ¡c hÆ¡n, Ä‘áº·c biá»‡t khi khuÃ´n máº·t
thay Ä‘á»•i theo thá»i gian:

1. Dual-Model Ensemble: ArcFace + Facenet512
2. Multi-Reference: LÆ°u tá»«ng embedding thay vÃ¬ chá»‰ average vector
3. Lightweight Classifier: sklearn SGDClassifier cho personalized decision boundary
4. Adaptive Update: CÃ³ thá»ƒ cáº­p nháº­t reference embeddings theo thá»i gian

Final Score = 0.5 * arcface_score + 0.3 * facenet_score + 0.2 * classifier_score
"""

import logging
import os
from pathlib import Path

import numpy as np
from deepface import DeepFace

from app.service.reference_store import (
    MODELS,
    DATA_DIR,
    ReferenceStore,
)
from app.service.classifier_service import PersonalizedClassifier

logger = logging.getLogger(__name__)

CLASSIFIER_WEIGHT = 0.2  # weight cho sklearn classifier score
DETECTOR_BACKEND = "retinaface"
FINAL_THRESHOLD = 0.50  # ngÆ°á»¡ng quyáº¿t Ä‘á»‹nh cuá»‘i cÃ¹ng (score < threshold â†’ MATCH)

VECTOR_DIR = "image/vector"
TEST_DIR = "image/test"


# â”€â”€ Helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


def ensure_dirs():
    """Táº¡o thÆ° má»¥c data náº¿u chÆ°a cÃ³."""
    Path(DATA_DIR).mkdir(parents=True, exist_ok=True)


def get_embedding(img_path: str, model_name: str) -> np.ndarray:
    """TrÃ­ch xuáº¥t face embedding tá»« áº£nh báº±ng model chá»‰ Ä‘á»‹nh."""
    result = DeepFace.represent(
        img_path=img_path,
        model_name=model_name,
        detector_backend=DETECTOR_BACKEND,
        enforce_detection=True,
    )
    emb = np.array(result[0]["embedding"])
    # Chuáº©n hoÃ¡ L2
    return emb / np.linalg.norm(emb)


def get_multi_model_embeddings(img_path: str) -> dict[str, np.ndarray]:
    """TrÃ­ch xuáº¥t embeddings tá»« táº¥t cáº£ models cho 1 áº£nh."""
    embeddings = {}
    for model_name in MODELS:
        embeddings[model_name] = get_embedding(img_path, model_name)
    return embeddings


# â”€â”€ Ensemble Scoring â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


def compute_final_score(
    ref_store: ReferenceStore,
    classifier: PersonalizedClassifier,
    test_embeddings: dict[str, np.ndarray],
) -> dict:
    """
    TÃ­nh Ä‘iá»ƒm tá»•ng há»£p tá»« ensemble.

    Returns dict vá»›i chi tiáº¿t tá»«ng model score + final score.
    """
    details = {}
    weighted_sum = 0.0

    # Distance scores tá»« tá»«ng model
    for model_name, config in MODELS.items():
        dist = ref_store.compute_distance(model_name, test_embeddings[model_name])
        weighted_sum += config["weight"] * dist
        details[model_name] = {
            "distance": dist,
            "threshold": config["threshold"],
            "weight": config["weight"],
            "match": dist < config["threshold"],
        }

    # Classifier score
    concat_emb = np.concatenate([test_embeddings[m] for m in MODELS])
    clf_score = classifier.predict_score(concat_emb)
    weighted_sum += CLASSIFIER_WEIGHT * clf_score
    details["classifier"] = {
        "score": clf_score,
        "weight": CLASSIFIER_WEIGHT,
        "match": clf_score < 0.5,
    }

    details["final_score"] = weighted_sum
    details["final_match"] = weighted_sum < FINAL_THRESHOLD
    return details


# â”€â”€ Pipeline Steps â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


def build_references(ref_store: ReferenceStore):
    """BÆ°á»›c 1: XÃ¢y dá»±ng reference embeddings tá»« áº£nh vector."""
    if ref_store.load():
        print(f"\n[1] ÄÃ£ load {ref_store.count()} reference embeddings tá»« cache")
        return

    print(f"\n[1] TrÃ­ch xuáº¥t reference embeddings tá»« '{VECTOR_DIR}':")
    for filename in sorted(os.listdir(VECTOR_DIR)):
        filepath = os.path.join(VECTOR_DIR, filename)
        if not os.path.isfile(filepath):
            continue
        print(f"  [ref] Äang xá»­ lÃ½: {filename}")
        try:
            embs = get_multi_model_embeddings(filepath)
            for model_name, emb in embs.items():
                ref_store.add(model_name, emb)
        except Exception as e:
            print(f"  [ref] Lá»—i vá»›i {filename}: {e}")

    if ref_store.count() == 0:
        raise RuntimeError("KhÃ´ng trÃ­ch xuáº¥t Ä‘Æ°á»£c embedding nÃ o!")

    print(
        f"  => ÄÃ£ trÃ­ch xuáº¥t embeddings tá»« {ref_store.count()} áº£nh "
        f"Ã— {len(MODELS)} models"
    )
    ref_store.save()


def build_classifier(classifier: PersonalizedClassifier, ref_store: ReferenceStore):
    """BÆ°á»›c 2: Train hoáº·c load personalized classifier."""
    if classifier.load():
        print("\n[2] ÄÃ£ load classifier tá»« cache")
        return

    print("\n[2] Train personalized classifier:")
    positive_features = ref_store.get_all_concatenated()
    classifier.train(positive_features)
    classifier.save()


def print_result(result: dict):
    """In chi tiáº¿t káº¿t quáº£ matching."""
    for model_name in MODELS:
        d = result[model_name]
        m = "âœ“" if d["match"] else "âœ—"
        print(
            f"    {model_name:12s}: dist={d['distance']:.4f} "
            f"(threshold={d['threshold']}) [{m}]"
        )

    cd = result["classifier"]
    cm = "âœ“" if cd["match"] else "âœ—"
    print(f"    {'Classifier':12s}: score={cd['score']:.4f} [{cm}]")

    status = "MATCH âœ“" if result["final_match"] else "NOT MATCH âœ—"
    print(f"    {'â”€' * 40}")
    print(
        f"    Final Score : {result['final_score']:.4f} (threshold: {FINAL_THRESHOLD})"
    )
    print(f"    Káº¿t quáº£     : {status}")


def run_tests(ref_store: ReferenceStore, classifier: PersonalizedClassifier):
    """BÆ°á»›c 3: Test matching."""
    print(f"\n[3] Kiá»ƒm tra face matching trong '{TEST_DIR}':")
    print("-" * 60)

    for filename in sorted(os.listdir(TEST_DIR)):
        filepath = os.path.join(TEST_DIR, filename)
        if not os.path.isfile(filepath):
            continue
        print(f"\n  ğŸ“· {filename}")
        try:
            test_embs = get_multi_model_embeddings(filepath)
            result = compute_final_score(ref_store, classifier, test_embs)
            print_result(result)
        except Exception as e:
            print(f"    Lá»—i: {e}")


# â”€â”€ CLI Commands â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


def cmd_match():
    """Lá»‡nh máº·c Ä‘á»‹nh: build references, train classifier, test matching."""
    ref_store = ReferenceStore()
    classifier = PersonalizedClassifier()

    build_references(ref_store)
    build_classifier(classifier, ref_store)
    run_tests(ref_store, classifier)


def cmd_retrain():
    """
    Train your own model: XoÃ¡ cache cÅ©, trÃ­ch xuáº¥t láº¡i embeddings tá»«
    image/vector/ vÃ  train classifier tá»« Ä‘áº§u.
    DÃ¹ng khi thÃªm/xoÃ¡ nhiá»u áº£nh reference hoáº·c muá»‘n reset.
    """
    print("\n[retrain] XoÃ¡ cache cÅ© vÃ  train láº¡i tá»« Ä‘áº§u...")
    for fname in ("references.json", "classifier.pkl"):
        fpath = os.path.join(DATA_DIR, fname)
        if os.path.exists(fpath):
            os.remove(fpath)
            print(f"  ÄÃ£ xoÃ¡ {fpath}")

    ref_store = ReferenceStore()
    classifier = PersonalizedClassifier()

    build_references(ref_store)
    build_classifier(classifier, ref_store)
    print("\n[retrain] HoÃ n táº¥t â€” model Ä‘Ã£ Ä‘Æ°á»£c train láº¡i.")


def cmd_enroll(image_paths: list[str]):
    """
    Online model updating: ThÃªm áº£nh má»›i vÃ o reference store vÃ 
    incremental update classifier (partial_fit) mÃ  khÃ´ng cáº§n retrain toÃ n bá»™.

    DÃ¹ng khi user thÃªm 1-2 áº£nh má»›i (vÃ­ dá»¥: áº£nh chá»¥p hÃ´m nay) Ä‘á»ƒ model
    thÃ­ch nghi vá»›i thay Ä‘á»•i khuÃ´n máº·t theo thá»i gian.
    """
    ref_store = ReferenceStore()
    classifier = PersonalizedClassifier()

    if not ref_store.load():
        print("[enroll] ChÆ°a cÃ³ reference store. Cháº¡y 'match' hoáº·c 'retrain' trÆ°á»›c.")
        return
    if not classifier.load():
        print("[enroll] ChÆ°a cÃ³ classifier. Cháº¡y 'match' hoáº·c 'retrain' trÆ°á»›c.")
        return

    old_count = ref_store.count()
    new_features = []

    for img_path in image_paths:
        if not os.path.isfile(img_path):
            print(f"  [enroll] File khÃ´ng tá»“n táº¡i: {img_path}")
            continue
        print(f"  [enroll] Äang xá»­ lÃ½: {img_path}")
        try:
            ref_store.enroll(img_path, get_multi_model_embeddings)
            # Láº¥y concatenated embedding cho classifier update
            embs = {m: ref_store.references[m][-1] for m in MODELS}
            concat = np.concatenate([embs[m] for m in MODELS])
            new_features.append(concat)
        except Exception as e:
            print(f"  [enroll] Lá»—i: {e}")

    if not new_features:
        print("[enroll] KhÃ´ng cÃ³ áº£nh nÃ o Ä‘Æ°á»£c thÃªm.")
        return

    # Incremental update classifier (partial_fit)
    new_features_arr = np.array(new_features)
    classifier.partial_update(new_features_arr)

    ref_store.save()
    classifier.save()

    print(f"\n[enroll] ÄÃ£ thÃªm {ref_store.count() - old_count} áº£nh má»›i ")
    print(f"  References: {old_count} â†’ {ref_store.count()}")
    print("  Classifier Ä‘Ã£ Ä‘Æ°á»£c cáº­p nháº­t (online update).")
